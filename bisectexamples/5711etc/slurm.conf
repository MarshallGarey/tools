# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine=voyager
# 
PropagateResourceLimits=NONE
#MailProg=/bin/mail 
MpiDefault=none
#MpiParams=ports=12000-12999

# Enable use of PAM to control access to nodes via ssh
#UsePAM=1

# Use cgroups to track processes.
ProctrackType=proctrack/cgroup

# Disable jobaccountgather memory limits.
MemLimitEnforce=no
JobAcctGatherParams=NoOverMemoryKill

# GRES
GresTypes=gpu

# Always return to service upon proving valid, regarless of reason for down state.
ReturnToService=2

SlurmctldPidFile=/home/marshall/slurm/17.11/voyager/run/slurmctld5517.pid
SlurmctldPort=3001
SlurmdPidFile=/home/marshall/slurm/17.11/voyager/run/slurmd5517-%n.pid
SlurmdPort=32999
SlurmdSpoolDir=/home/marshall/slurm/17.11/voyager/spool/slurmd5517-%n
SlurmUser=marshall
SlurmdUser=root
SlurmdTimeout=1200
StateSaveLocation=/home/marshall/slurm/17.11/voyager/state5711
SwitchType=switch/none

# Use cgroups to manage Tasks.
TaskPlugin=task/affinity,task/cgroup

#RebootProgram="/usr/sbin/reboot"

# TIMERS 
#KillWait=30 
#MinJobAge=300 
#SlurmctldTimeout=120 
#SlurmdTimeout=300 
BatchStartTimeout=30
EioTimeout=120
MessageTimeout=60
UnkillableStepTimeout=240
 
# 
# SCHEDULING 
FastSchedule=2
SchedulerType=sched/backfill
SchedulerParameters=kill_invalid_depend
#SchedulerPort=7321 
SelectType=select/cons_res
# Schedule based on core and memory.
SelectTypeParameters=CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK,CR_ONE_TASK_PER_CORE
#SelectTypeParameters=CR_Core_Memory
# Set a default limit on each job of 2 GB/CPU, where CPU == core due to CR_Core_Memory above.
DefMemPerCPU=2048

# This lets jobs continue after reservation expires.
ResvOverRun=UNLIMITED

# Prioirty calculations.
PriorityType=priority/multifactor
PriorityFlags=FAIR_TREE
PriorityDecayHalfLife=60-0
PriorityFavorSmall=NO
PriorityMaxAge=60-0
PriorityWeightAge=10
PriorityWeightFairshare=75000
PriorityWeightJobSize=1000
PriorityWeightPartition=1
PriorityWeightQOS=10000
# 
# 
TopologyPlugin=topology/none
RoutePlugin=route/default

# LOGGING AND ACCOUNTING 
AccountingStorageEnforce=associations,limits,qos
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageTRES=gres/gpu
AccountingStoragePort=32997

ClusterName=voyager


#AcctGatherNodeFreq=300
#AcctGatherEnergyType=acct_gather_energy/none
#AcctGatherInfinibandType=acct_gather_infiniband/ofed
#AcctGatherFilesystemType=acct_gather_filesystem/none
#AcctGatherProfileType=acct_gather_profile/hdf5


JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/cgroup
SlurmctldDebug=info
#SlurmctldSyslogDebug=info
#SlurmctldLogFile=/var/spool/slurm/log/slurmctld.log
SlurmctldLogFile=/home/marshall/slurm/17.11/voyager/log/slurmctld5711.log
SlurmdDebug=info
#SlurmdSyslogDebug=info
#SlurmdLogFile=/var/spool/slurm/log/slurmd.log
#DebugFlags=Gres
# SlurmSchedLogLevel=1
# SlurmSchedLogFile=/var/spool/slurm/log/slurmsched.log
# 
# 
# JobCompType=jobcomp/filetxt
# JobCompLoc=/var/spool/slurm/log/slurm_jobcomp.log

#TmpFS=/local/scratch

# Increase timeout for credentials, to defeat CPU spinning on locks randomly targeting slurmd.
AuthInfo=cred_expire=300

# Prolog and Epilog Scripts
#Include scripts.conf

# Health Check
#HealthCheckProgram=/usr/sbin/nhc
#HealthCheckNodeState=CYCLE
#HealthCheckInterval=300

MaxArraySize=1048576
MaxJobCount=2097152

# Submit Plugins.
#JobSubmitPlugins=lua

# Include node configs.
Include nodes.conf
# Include partition configs.
Include partitions.conf

